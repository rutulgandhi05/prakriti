pretrained_model_name_or_path: stabilityai/stable-diffusion-xl-base-1.0
pretrained_vae_model_name_or_path: madebyollin/sdxl-vae-fp16-fix

inference_prompt: Create a hyper-realistic, high-resolution portrait of human male, a wise and gentle character with a calm and knowing expression, has piercing, brown eyes that convey intelligence and warmth, framed by a well-groomed, soft white beard with subtle fine wrinkles around the eyes and mouth. His face is softly lit with natural, warm sunlight, casting gentle shadows that enhance his facial structure, wears a simple but elegant dark green robe with intricate, gold-embroidered trim along the edges, reflecting a subtle sense of wisdom and nobility. The robe is made of a soft, textured fabric that falls gracefully over his shoulders, catching soft highlights in the ambient light. His posture is relaxed but dignified, slightly leaning forward as if in quiet conversation.  The background is softly out of focus to keep the attention on face and expression, enhancing the sense of depth and realism in the portrait. Lighting: Gentle, diffused sunlight that creates a soft glow, illuminating his face and adding a hint of warmth. The light source is slightly angled, creating subtle shadows on one side of his face, adding depth while maintaining the softness of his features. Photo Style: Ultra-realistic, high-quality photography with fine details, sharp focus on the face and eyes, soft bokeh background, professional studio quality. Captured in 4K resolution for fine detail, with natural color tones and high dynamic range (HDR).

learnable_property: object
initializer_token: cha
placeholder_token: e<r$i>n
repeats: 1
save_as_full_pipeline: true
validation_prompt: A photo of e<r$i>n 
caption_column: text
resolution: 1024
random_flip: true
train_batch_size: 1
num_train_epochs: 1
checkpointing_steps: 500
learning_rate: 0.00003
lr_scheduler: constant
lr_warmup_steps: 0
mixed_precision: bf16
seed: 45
character_name: erin
output_dir: data/erin/experiments
train_data_dir: data/erin/loop_images
backup_data_dir_root: data/erin  # Your dataset backup folder
# lora_ckpt_dir: checkpoint-1200
max_train_steps: 500
num_of_generated_img: 128
dmin_c: 10
dsize_c: 20         
infer_steps: 35
adam_epsilon: 0.000000001
adam_weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.99
max_loop: 5
convergence_scale: 0.85  # 80% in the paper

# Enhancements
high_learning_rate: 0.00003     # Initial high learning rate
low_learning_rate: 0.000005     # Lower learning rate after consistency threshold
consistency_threshold: 0.9     # Character consistency threshold
use_dinov2_clip: true           # Use both DINOv2 and CLIP models for embedding extraction
use_dual_tokenizers: true
character_consistency_threshold: 0.85  # Consistency level required to be considered converged
cluster_specific_finetune: true  # Enable fine-tuning for individual clusters

# inherited from origin, no need to change
gradient_accumulation_steps: 1
report_to: tensorboard
push_to_hub: false
num_vectors: 1
enable_xformers_memory_efficient_attention: false
rank: 4
train_text_encoder: false
allow_tf32: false
scale_lr: false
use_8bit_adam: false
text_inv: true  # Enable textual inversion fine-tuning
lora: true      # Enable LoRA fine-tuning
center_crop: true
dataloader_num_workers: 0
noise_offset: 0
max_grad_norm: 1.0
num_validation_images: 4
validation_epochs: 1
