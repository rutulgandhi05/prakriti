pretrained_model_name_or_path: stabilityai/stable-diffusion-xl-base-1.0
pretrained_vae_model_name_or_path: madebyollin/sdxl-vae-fp16-fix

inference_prompt: photo of a handsome 52yo german man with straight defined jawline, hazel eyes, fine wrinkles, (naked:1.4), fit body, shoulder length hair, (neutral gray background:1.3)
text_inv_prompt: photo of 3r1n naked, neutral gray background
negative_prompt: (glasses:1.2), young, teen, child, (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation, tattoo
learnable_property: object
initializer_token: man
placeholder_token: 3r1n
repeats: 1
save_as_full_pipeline: true
validation_prompt: an closeup color photo of 3r1n in a forest
caption_column: text
resolution: 1024
random_flip: true
train_batch_size: 1
num_train_epochs: 1
checkpointing_steps: 500
learning_rate: 0.00003
lr_scheduler: constant
lr_warmup_steps: 0
mixed_precision: bf16
seed: 45
character_name: 3r1n
output_dir: data/3r1n_mod1/experiments
train_data_dir: data/3r1n_mod1/loop_images
backup_data_dir_root: data/3r1n_final_simple  # Your dataset backup folder
# lora_ckpt_dir: checkpoint-1200
max_train_steps: 500
num_of_generated_img: 201
dmin_c: 5
dsize_c: 15         
infer_steps: 55
adam_epsilon: 0.000000001
adam_weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.99
max_loop: 5
convergence_scale: 0.90  # 80% in the paper
min_samples: 1

# Enhancements
high_learning_rate: 0.00003     # Initial high learning rate
low_learning_rate: 0.000005     # Lower learning rate after consistency threshold
consistency_threshold: 0.9     # Character consistency threshold
use_dinov2_clip: true           # Use both DINOv2 and CLIP models for embedding extraction
use_dual_tokenizers: true
character_consistency_threshold: 0.85  # Consistency level required to be considered converged
cluster_specific_finetune: true  # Enable fine-tuning for individual clusters

# inherited from origin, no need to change
gradient_accumulation_steps: 1
report_to: tensorboard
push_to_hub: false
num_vectors: 1
enable_xformers_memory_efficient_attention: false
rank: 4
train_text_encoder: false
allow_tf32: false
scale_lr: false
use_8bit_adam: false
text_inv: true  # Enable textual inversion fine-tuning
lora: false      # Enable LoRA fine-tuning
center_crop: true
dataloader_num_workers: 0
noise_offset: 0
max_grad_norm: 1.0
num_validation_images: 4
validation_epochs: 1
eps: 0.5

teacher_output_dir: data/3r1n_mod/teacher/experiments
teacher_train_data_dir: data/3r1n_mod/teacher/loop_images
teacher_backup_data_dir_root: data/3r1n_mod/teacher